"""
Miner Anomaly Detection System - Phase A: Rules + Robust Statistics + Output Layer

Deterministic baseline tracking and rule-based anomaly detection.
ML (Isolation Forest) in Phase B.
Canonical MinerHealth output for consumption.
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from sqlalchemy import select, func, and_
from sqlalchemy.ext.asyncio import AsyncSession
import statistics

from core.database import (
    AsyncSessionLocal, Miner, Telemetry, MinerBaseline, HealthEvent, MinerHealthCurrent
)

logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION
# ============================================================================

# Baseline windows
BASELINE_WINDOW_24H = 24  # hours
BASELINE_WINDOW_7D = 168  # hours

# Anomaly thresholds (percentages)
THRESHOLD_HASHRATE_DROP = 15  # %
THRESHOLD_EFFICIENCY_DRIFT = 20  # % increase in W/TH
THRESHOLD_TEMP_MARGIN = 10  # degrees C above baseline
THRESHOLD_REJECT_RATE = 5  # % reject rate
THRESHOLD_POWER_SPIKE = 15  # % increase without hashrate increase

# Health score weights
WEIGHT_HASHRATE = 30
WEIGHT_EFFICIENCY = 25
WEIGHT_TEMPERATURE = 20
WEIGHT_REJECTS = 15
WEIGHT_POWER = 10

# Health status thresholds
STATUS_HEALTHY_MIN = 80  # 80-100
STATUS_WARNING_MIN = 50  # 50-79
# 0-49 = critical

# Minimum data requirements
MIN_SAMPLES_FOR_BASELINE = 60  # Need at least 1 hour of data

# ============================================================================
# REASON CODES & SUGGESTED ACTIONS
# ============================================================================

# Reason codes (enum-like constants)
REASON_HASHRATE_DROP = "HASHRATE_DROP"
REASON_EFFICIENCY_DRIFT = "EFFICIENCY_DRIFT"
REASON_TEMP_HIGH = "TEMP_HIGH"
REASON_REJECT_RATE_SPIKE = "REJECT_RATE_SPIKE"
REASON_POWER_SPIKE = "POWER_SPIKE"
REASON_SENSOR_MISSING = "SENSOR_MISSING"
REASON_INSUFFICIENT_DATA = "INSUFFICIENT_DATA"

# Severity levels
SEVERITY_INFO = "info"
SEVERITY_WARNING = "warning"
SEVERITY_CRITICAL = "critical"

# Suggested actions (enum-like constants)
ACTION_RESTART_MINER = "RESTART_MINER"
ACTION_DROP_MODE = "DROP_MODE"
ACTION_SWITCH_POOL = "SWITCH_POOL"
ACTION_CHECK_NETWORK = "CHECK_NETWORK"
ACTION_CHECK_COOLING = "CHECK_COOLING"
ACTION_CHECK_PSU = "CHECK_PSU"
ACTION_WAIT_FOR_BASELINE = "WAIT_FOR_BASELINE"

# Reason code â†’ suggested actions mapping (deterministic)
REASON_TO_ACTIONS = {
    REASON_HASHRATE_DROP: [ACTION_RESTART_MINER, ACTION_CHECK_NETWORK, ACTION_CHECK_PSU],
    REASON_EFFICIENCY_DRIFT: [ACTION_DROP_MODE, ACTION_CHECK_PSU, ACTION_CHECK_COOLING],
    REASON_TEMP_HIGH: [ACTION_CHECK_COOLING, ACTION_DROP_MODE],
    REASON_REJECT_RATE_SPIKE: [ACTION_SWITCH_POOL, ACTION_CHECK_NETWORK],
    REASON_POWER_SPIKE: [ACTION_CHECK_PSU, ACTION_RESTART_MINER],
    REASON_SENSOR_MISSING: [ACTION_CHECK_NETWORK, ACTION_RESTART_MINER],
    REASON_INSUFFICIENT_DATA: [ACTION_WAIT_FOR_BASELINE],
}


# ============================================================================
# ROBUST STATISTICS
# ============================================================================

def calculate_median_mad(values: List[float]) -> Tuple[float, float]:
    """
    Calculate median and MAD (Median Absolute Deviation).
    More robust to outliers than mean/std.
    
    Returns:
        (median, mad)
    """
    if not values:
        return (0.0, 0.0)
    
    median = statistics.median(values)
    absolute_deviations = [abs(x - median) for x in values]
    mad = statistics.median(absolute_deviations)
    
    return (median, mad)


def is_anomalous(current_value: float, median: float, mad: float, threshold_factor: float = 3.0) -> bool:
    """
    Check if value is anomalous using MAD-based threshold.
    
    Args:
        current_value: Current metric value
        median: Baseline median
        mad: Baseline MAD
        threshold_factor: How many MADs away = anomaly (default 3.0)
    
    Returns:
        True if anomalous
    """
    if mad == 0:
        # No variance in baseline - any deviation is suspicious
        return abs(current_value - median) > (median * 0.01)  # 1% tolerance
    
    deviation = abs(current_value - median)
    return deviation > (threshold_factor * mad)


# ============================================================================
# BASELINE CALCULATION
# ============================================================================

async def compute_baselines_for_miner(
    db: AsyncSession,
    miner_id: int,
    window_hours: int = BASELINE_WINDOW_24H
) -> Dict[str, Tuple[float, float]]:
    """
    Compute robust baselines for a miner using median + MAD.
    
    Returns:
        Dict[metric_name, (median, mad)]
    """
    logger.info(f"Computing {window_hours}h baselines for miner {miner_id}")
    
    # Get miner to check type
    result = await db.execute(select(Miner).where(Miner.id == miner_id))
    miner = result.scalar_one_or_none()
    if not miner:
        logger.warning(f"Miner {miner_id} not found")
        return {}
    
    # Get recent telemetry
    cutoff = datetime.utcnow() - timedelta(hours=window_hours)
    result = await db.execute(
        select(Telemetry)
        .where(
            and_(
                Telemetry.miner_id == miner_id,
                Telemetry.timestamp >= cutoff,
                Telemetry.hashrate.isnot(None),
                Telemetry.hashrate > 0
            )
        )
        .order_by(Telemetry.timestamp.desc())
    )
    telemetry_records = result.scalars().all()
    
    if len(telemetry_records) < MIN_SAMPLES_FOR_BASELINE:
        logger.warning(f"Insufficient data for miner {miner_id}: {len(telemetry_records)} samples")
        return {}
    
    # Extract metrics per mode (if mode tracking exists)
    metrics_by_mode: Dict[Optional[str], Dict[str, List[float]]] = {}
    
    for record in telemetry_records:
        mode = record.mode  # Can be None
        
        if mode not in metrics_by_mode:
            metrics_by_mode[mode] = {
                "hashrate_mean": [],
                "power_mean": [],
                "w_per_th": [],
                "temp_mean": [],
                "reject_rate": []
            }
        
        # Hashrate
        if record.hashrate and record.hashrate > 0:
            # Convert to TH/s for consistency
            hashrate_ths = _convert_to_ths(record.hashrate, record.hashrate_unit or "GH/s")
            metrics_by_mode[mode]["hashrate_mean"].append(hashrate_ths)
            
            # W/TH (only if we have power)
            if record.power_watts and record.power_watts > 0:
                w_per_th = record.power_watts / hashrate_ths
                metrics_by_mode[mode]["w_per_th"].append(w_per_th)
                metrics_by_mode[mode]["power_mean"].append(record.power_watts)
        
        # Temperature
        if record.temperature:
            metrics_by_mode[mode]["temp_mean"].append(record.temperature)
        
        # Reject rate
        if record.shares_accepted is not None and record.shares_rejected is not None:
            total_shares = record.shares_accepted + record.shares_rejected
            if total_shares > 0:
                reject_rate = (record.shares_rejected / total_shares) * 100
                metrics_by_mode[mode]["reject_rate"].append(reject_rate)
    
    # Calculate baselines for each mode
    baselines = {}
    
    for mode, metrics in metrics_by_mode.items():
        for metric_name, values in metrics.items():
            if len(values) >= MIN_SAMPLES_FOR_BASELINE:
                median, mad = calculate_median_mad(values)
                baselines[(mode, metric_name)] = {
                    "median": median,
                    "mad": mad,
                    "samples": len(values)
                }
                
                logger.info(
                    f"Miner {miner_id} [{mode if mode else 'None'}] {metric_name}: "
                    f"median={median:.2f}, mad={mad:.2f}, samples={len(values)}"
                )
    
    return baselines


async def update_baselines_for_all_miners(db: AsyncSession):
    """Update baselines for all enabled miners"""
    logger.info("Updating baselines for all miners")
    
    result = await db.execute(
        select(Miner).where(Miner.enabled == True)
    )
    miners = result.scalars().all()
    
    for miner in miners:
        # Compute 24h baselines
        baselines_24h = await compute_baselines_for_miner(db, miner.id, BASELINE_WINDOW_24H)
        
        # Store in database
        for (mode_key, metric_name), stats in baselines_24h.items():
            median = stats["median"]
            mad = stats["mad"]
            sample_count = stats["samples"]
            
            # Upsert baseline
            result = await db.execute(
                select(MinerBaseline)
                .where(
                    and_(
                        MinerBaseline.miner_id == miner.id,
                        MinerBaseline.mode == mode_key,
                        MinerBaseline.metric_name == metric_name,
                        MinerBaseline.window_hours == BASELINE_WINDOW_24H
                    )
                )
            )
            existing = result.scalar_one_or_none()
            
            if existing:
                existing.median_value = median
                existing.mad_value = mad
                existing.sample_count = sample_count
                existing.updated_at = datetime.utcnow()
            else:
                baseline = MinerBaseline(
                    miner_id=miner.id,
                    mode=mode_key,
                    metric_name=metric_name,
                    median_value=median,
                    mad_value=mad,
                    sample_count=sample_count,
                    window_hours=BASELINE_WINDOW_24H
                )
                db.add(baseline)
    
    await db.commit()
    logger.info("Baseline update complete")


# ============================================================================
# RULE-BASED ANOMALY DETECTION
# ============================================================================

def _build_reason(
    code: str,
    severity: str,
    metric: str,
    actual: float,
    expected_min: float,
    expected_max: float,
    unit: str,
    delta_pct: Optional[float] = None
) -> Dict:
    """
    Build a structured reason object (canonical format).
    
    Args:
        code: Reason code (e.g. HASHRATE_DROP)
        severity: info, warning, or critical
        metric: Metric name (e.g. hashrate_th, w_per_th)
        actual: Current value
        expected_min: Expected minimum (baseline - tolerance)
        expected_max: Expected maximum (baseline + tolerance)
        unit: Unit string (TH/s, W/TH, C, %)
        delta_pct: Percentage delta (optional)
    
    Returns:
        Structured reason dict
    """
    reason = {
        "code": code,
        "severity": severity,
        "metric": metric,
        "actual": round(actual, 2),
        "expected_min": round(expected_min, 2),
        "expected_max": round(expected_max, 2),
        "unit": unit
    }
    
    if delta_pct is not None:
        reason["delta_pct"] = round(delta_pct, 1)
    
    return reason


def _calculate_status(health_score: int) -> str:
    """
    Calculate status from health score (deterministic mapping).
    
    Args:
        health_score: 0-100
    
    Returns:
        'healthy', 'warning', or 'critical'
    """
    if health_score >= STATUS_HEALTHY_MIN:
        return "healthy"
    elif health_score >= STATUS_WARNING_MIN:
        return "warning"
    else:
        return "critical"


def _derive_suggested_actions(reason_codes: List[str]) -> List[str]:
    """
    Derive suggested actions from reason codes (deterministic lookup).
    
    Args:
        reason_codes: List of reason code strings
    
    Returns:
        Deduplicated list of action enums
    """
    actions = set()
    
    for code in reason_codes:
        if code in REASON_TO_ACTIONS:
            actions.update(REASON_TO_ACTIONS[code])
    
    # Return as sorted list for consistency
    return sorted(list(actions))


# ============================================================================
# RULE-BASED ANOMALY DETECTION
# ============================================================================

async def check_miner_health(db: AsyncSession, miner_id: int) -> Optional[Dict]:
    """
    Check miner health using deterministic rules and produce canonical MinerHealth object.
    
    Returns:
        {
            "miner_id": int,
            "timestamp": datetime,
            "health_score": int (0-100),
            "status": str (healthy/warning/critical),
            "anomaly_score": float (0-1, nullable),
            "reasons": List[Dict],  # Structured reason objects
            "suggested_actions": List[str],  # Action enums
            "mode": str (nullable)
        }
    """
    # Get miner (MUST exist - hard invariant)
    result = await db.execute(select(Miner).where(Miner.id == miner_id))
    miner = result.scalar_one_or_none()
    if not miner or not miner.enabled:
        return None
    
    # Get latest telemetry (last 5 minutes)
    cutoff = datetime.utcnow() - timedelta(minutes=5)
    result = await db.execute(
        select(Telemetry)
        .where(
            and_(
                Telemetry.miner_id == miner_id,
                Telemetry.timestamp >= cutoff
            )
        )
        .order_by(Telemetry.timestamp.desc())
    )
    recent_telemetry = result.scalars().all()
    
    # Hard invariant: if no telemetry, emit SENSOR_MISSING with critical status
    if not recent_telemetry:
        logger.warning(f"No recent telemetry for miner {miner_id}")
        reason = _build_reason(
            code=REASON_SENSOR_MISSING,
            severity=SEVERITY_CRITICAL,
            metric="telemetry",
            actual=0,
            expected_min=1,
            expected_max=999,
            unit="records"
        )
        
        return {
            "miner_id": miner_id,
            "timestamp": datetime.utcnow(),
            "health_score": 0,
            "status": "critical",
            "anomaly_score": None,
            "reasons": [reason],
            "suggested_actions": _derive_suggested_actions([REASON_SENSOR_MISSING]),
            "mode": None
        }
    
    # Get current mode
    current_mode = recent_telemetry[0].mode
    
    # Get baselines for this mode
    result = await db.execute(
        select(MinerBaseline)
        .where(
            and_(
                MinerBaseline.miner_id == miner_id,
                MinerBaseline.mode == current_mode,
                MinerBaseline.window_hours == BASELINE_WINDOW_24H
            )
        )
    )
    baselines_records = result.scalars().all()
    
    baselines = {
        b.metric_name: (b.median_value, b.mad_value)
        for b in baselines_records
    }
    
    # If no baselines, return warning state
    if not baselines:
        logger.warning(f"No baselines for miner {miner_id} mode {current_mode}")
        
        # Count how many telemetry samples we actually have (last 24h)
        telemetry_cutoff = datetime.utcnow() - timedelta(hours=BASELINE_WINDOW_24H)
        result = await db.execute(
            select(func.count(Telemetry.id))
            .where(
                and_(
                    Telemetry.miner_id == miner_id,
                    Telemetry.timestamp >= telemetry_cutoff
                )
            )
        )
        actual_sample_count = result.scalar() or 0
        
        reason = _build_reason(
            code=REASON_INSUFFICIENT_DATA,
            severity=SEVERITY_WARNING,
            metric="baseline_samples",
            actual=actual_sample_count,
            expected_min=MIN_SAMPLES_FOR_BASELINE,
            expected_max=999999,
            unit="samples"
        )
        
        return {
            "miner_id": miner_id,
            "timestamp": datetime.utcnow(),
            "health_score": 50,
            "status": "warning",
            "anomaly_score": None,
            "reasons": [reason],
            "suggested_actions": _derive_suggested_actions([REASON_INSUFFICIENT_DATA]),
            "mode": current_mode
        }
    
    # Calculate current metrics (with unit conversions)
    hashrates = [_convert_to_ths(t.hashrate, t.hashrate_unit or "GH/s") 
                 for t in recent_telemetry if t.hashrate and t.hashrate > 0]
    powers = [t.power_watts for t in recent_telemetry if t.power_watts and t.power_watts > 0]
    temps = [t.temperature for t in recent_telemetry if t.temperature]
    
    current_hashrate = statistics.mean(hashrates) if hashrates else None
    current_power = statistics.mean(powers) if powers else None
    current_temp = statistics.mean(temps) if temps else None
    
    # Hard invariant: W/TH calculation (efficiency MUST be W/TH, never W/GH)
    current_w_per_th = None
    if current_power and current_hashrate and current_hashrate > 0:
        current_w_per_th = current_power / current_hashrate  # Power in W, hashrate in TH/s
    
    # Calculate reject rate
    total_accepted = sum(t.shares_accepted or 0 for t in recent_telemetry)
    total_rejected = sum(t.shares_rejected or 0 for t in recent_telemetry)
    total_shares = total_accepted + total_rejected
    current_reject_rate = (total_rejected / total_shares * 100) if total_shares > 0 else 0
    
    # Run checks and build structured reasons
    reasons = []
    health_score = 100.0
    
    # Check 1: Hashrate drop
    if current_hashrate and "hashrate_mean" in baselines:
        median_hr, mad_hr = baselines["hashrate_mean"]
        expected_min = max(0, median_hr - (3 * mad_hr))  # 3-MAD threshold (never negative)
        expected_max = median_hr + (3 * mad_hr)
        
        # Check if current hashrate is outside expected range
        if current_hashrate < expected_min:
            drop_pct = ((median_hr - current_hashrate) / median_hr) * 100 if median_hr > 0 else 0
            reason = _build_reason(
                code=REASON_HASHRATE_DROP,
                severity=SEVERITY_CRITICAL if drop_pct > 30 else SEVERITY_WARNING,
                metric="hashrate_th",
                actual=current_hashrate,
                expected_min=expected_min,
                expected_max=expected_max,
                unit="TH/s",
                delta_pct=-drop_pct
            )
            reasons.append(reason)
            health_score -= WEIGHT_HASHRATE * (drop_pct / 100)
    
    # Check 2: Efficiency drift (W/TH)
    if current_w_per_th and "w_per_th" in baselines:
        median_eff, mad_eff = baselines["w_per_th"]
        expected_min = max(0, median_eff - (3 * mad_eff))  # Never negative
        expected_max = median_eff + (3 * mad_eff)
        
        # Check if current efficiency is outside expected range (higher is worse for W/TH)
        if current_w_per_th > expected_max:
            drift_pct = ((current_w_per_th - median_eff) / median_eff) * 100 if median_eff > 0 else 0
            reason = _build_reason(
                code=REASON_EFFICIENCY_DRIFT,
                severity=SEVERITY_WARNING,
                metric="w_per_th",
                actual=current_w_per_th,
                expected_min=expected_min,
                expected_max=expected_max,
                unit="W/TH",
                delta_pct=drift_pct
            )
            reasons.append(reason)
            health_score -= WEIGHT_EFFICIENCY * (drift_pct / 100)
    
    # Check 3: Temperature
    if current_temp and "temp_mean" in baselines:
        median_temp, mad_temp = baselines["temp_mean"]
        temp_excess = current_temp - median_temp
        
        if temp_excess > THRESHOLD_TEMP_MARGIN:
            reason = _build_reason(
                code=REASON_TEMP_HIGH,
                severity=SEVERITY_CRITICAL if temp_excess > 20 else SEVERITY_WARNING,
                metric="temp_c",
                actual=current_temp,
                expected_min=median_temp - (3 * mad_temp),
                expected_max=median_temp + THRESHOLD_TEMP_MARGIN,
                unit="C",
                delta_pct=(temp_excess / median_temp * 100) if median_temp > 0 else None
            )
            reasons.append(reason)
            health_score -= WEIGHT_TEMPERATURE * (temp_excess / 100)
    
    # Check 4: Reject rate
    if current_reject_rate > THRESHOLD_REJECT_RATE:
        reason = _build_reason(
            code=REASON_REJECT_RATE_SPIKE,
            severity=SEVERITY_WARNING,
            metric="reject_rate",
            actual=current_reject_rate,
            expected_min=0,
            expected_max=THRESHOLD_REJECT_RATE,
            unit="%",
            delta_pct=current_reject_rate
        )
        reasons.append(reason)
        health_score -= WEIGHT_REJECTS * (current_reject_rate / 100)
    
    # Check 5: Power spike without hashrate increase
    if current_power and current_hashrate and "power_mean" in baselines and "hashrate_mean" in baselines:
        median_power, _ = baselines["power_mean"]
        median_hr, _ = baselines["hashrate_mean"]
        
        power_increase_pct = ((current_power - median_power) / median_power) * 100 if median_power > 0 else 0
        hashrate_change_pct = ((current_hashrate - median_hr) / median_hr) * 100 if median_hr > 0 else 0
        
        if power_increase_pct > THRESHOLD_POWER_SPIKE and hashrate_change_pct < 5:
            reason = _build_reason(
                code=REASON_POWER_SPIKE,
                severity=SEVERITY_WARNING,
                metric="power_w",
                actual=current_power,
                expected_min=median_power - (3 * mad_hr),  # Using hashrate MAD as proxy
                expected_max=median_power + (3 * mad_hr),
                unit="W",
                delta_pct=power_increase_pct
            )
            reasons.append(reason)
            health_score -= WEIGHT_POWER * (power_increase_pct / 100)
    
    # Clamp health score (integer output)
    health_score = int(max(0, min(100, health_score)))
    
    # Calculate status from health score
    status = _calculate_status(health_score)
    
    # Derive suggested actions from reason codes
    reason_codes = [r["code"] for r in reasons]
    suggested_actions = _derive_suggested_actions(reason_codes)
    
    return {
        "miner_id": miner_id,
        "timestamp": datetime.utcnow(),
        "health_score": health_score,
        "status": status,
        "anomaly_score": None,  # Will be populated by check_all_miners_health with ML score
        "reasons": reasons,
        "suggested_actions": suggested_actions,
        "mode": current_mode
    }


# ============================================================================
# FLEET HEALTH CHECK
# ============================================================================

async def check_all_miners_health(db: AsyncSession):
    """Check health for all enabled miners and persist canonical MinerHealth to MinerHealthCurrent"""
    logger.info("Checking health for all miners")
    
    result = await db.execute(
        select(Miner).where(Miner.enabled == True)
    )
    miners = result.scalars().all()
    
    for miner in miners:
        health_data = await check_miner_health(db, miner.id)
        
        if health_data:
            # Get ML anomaly score (Phase B)
            from core.ml_anomaly import predict_anomaly_score
            
            # Get recent telemetry for ML scoring
            cutoff = datetime.utcnow() - timedelta(minutes=5)
            result = await db.execute(
                select(Telemetry)
                .where(
                    and_(
                        Telemetry.miner_id == miner.id,
                        Telemetry.timestamp >= cutoff
                    )
                )
                .order_by(Telemetry.timestamp.desc())
            )
            recent_telemetry = result.scalars().all()
            
            ml_score = await predict_anomaly_score(db, miner.id, recent_telemetry)
            health_data["anomaly_score"] = ml_score  # Add ML score to canonical object
            
            # Persist to MinerHealthCurrent (upsert pattern: one row per miner)
            result = await db.execute(
                select(MinerHealthCurrent).where(MinerHealthCurrent.miner_id == miner.id)
            )
            current = result.scalar_one_or_none()
            
            if current:
                # Update existing row
                current.timestamp = health_data["timestamp"]
                current.health_score = health_data["health_score"]
                current.status = health_data["status"]
                current.anomaly_score = ml_score
                current.reasons = health_data["reasons"]
                current.suggested_actions = health_data["suggested_actions"]
                current.mode = health_data["mode"]
                current.updated_at = datetime.utcnow()
            else:
                # Insert new row
                current = MinerHealthCurrent(
                    miner_id=miner.id,
                    timestamp=health_data["timestamp"],
                    health_score=health_data["health_score"],
                    status=health_data["status"],
                    anomaly_score=ml_score,
                    reasons=health_data["reasons"],
                    suggested_actions=health_data["suggested_actions"],
                    mode=health_data["mode"]
                )
                db.add(current)
            
            # Store historical event (with new columns for Phase C)
            event = HealthEvent(
                miner_id=miner.id,
                health_score=health_data["health_score"],
                reasons=health_data["reasons"],
                mode=health_data["mode"],
                anomaly_score=ml_score,
                status=health_data["status"],
                suggested_actions=health_data["suggested_actions"]
            )
            db.add(event)
            
            # Log warnings for unhealthy miners
            if health_data["status"] in ["warning", "critical"] or (ml_score and ml_score > 0.7):
                reason_codes = [r["code"] for r in health_data["reasons"]]
                ml_score_str = f"{ml_score:.2f}" if ml_score is not None else "N/A"
                logger.warning(
                    f"Miner {miner.name} (ID {miner.id}) - Health: {health_data['health_score']}/100 "
                    f"Status: {health_data['status'].upper()} - ML: {ml_score_str} - "
                    f"Issues: {', '.join(reason_codes)} - Actions: {', '.join(health_data['suggested_actions'])}"
                )
    
    await db.commit()
    logger.info("Health check complete")


# ============================================================================
# UTILITIES
# ============================================================================

def _convert_to_ths(hashrate: float, unit: str) -> float:
    """Convert hashrate to TH/s"""
    unit = unit.upper()
    if "KH" in unit:
        return hashrate / 1_000_000_000
    elif "MH" in unit:
        return hashrate / 1_000_000
    elif "GH" in unit:
        return hashrate / 1_000
    elif "TH" in unit:
        return hashrate
    else:
        return hashrate / 1_000  # Assume GH/s if unknown
