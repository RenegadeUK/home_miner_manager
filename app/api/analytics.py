"""
Analytics API endpoints
"""
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Tuple
from pydantic import BaseModel
import io
import csv

from core.database import get_db, Miner, Telemetry, HealthScore
from core.health import HealthScoringService


router = APIRouter()

# Simple in-memory cache for CKPool analytics (5-minute TTL)
_ckpool_analytics_cache: Dict[str, Tuple[datetime, dict]] = {}


class HealthScoreResponse(BaseModel):
    overall_score: float
    uptime_score: float
    temperature_score: Optional[float] = None  # Optional for XMRig miners without temp sensors
    hashrate_score: float
    reject_rate_score: float
    data_points: int
    period_hours: int


class TelemetryStatsResponse(BaseModel):
    avg_hashrate: Optional[float]
    min_hashrate: Optional[float]
    max_hashrate: Optional[float]
    hashrate_unit: Optional[str]
    avg_temperature: Optional[float]
    max_temperature: Optional[float]
    avg_power: Optional[float]
    total_accepted: Optional[int]
    total_rejected: Optional[int]
    reject_rate: Optional[float]
    uptime_percent: float
    data_points: int


@router.get("/miners/{miner_id}/health", response_model=HealthScoreResponse)
async def get_miner_health(
    miner_id: int,
    hours: int = 24,
    db: AsyncSession = Depends(get_db)
):
    """Get current health score for a miner"""
    # Check miner exists
    result = await db.execute(select(Miner).where(Miner.id == miner_id))
    miner = result.scalar_one_or_none()
    
    if not miner:
        raise HTTPException(status_code=404, detail="Miner not found")
    
    score_data = await HealthScoringService.calculate_health_score(miner_id, db, hours)
    
    if not score_data:
        raise HTTPException(status_code=404, detail="Insufficient data to calculate health score")
    
    return score_data


@router.get("/miners/{miner_id}/health/trend")
async def get_miner_health_trend(
    miner_id: int,
    days: int = 7,
    db: AsyncSession = Depends(get_db)
):
    """Get health score trend over time"""
    result = await db.execute(select(Miner).where(Miner.id == miner_id))
    miner = result.scalar_one_or_none()
    
    if not miner:
        raise HTTPException(status_code=404, detail="Miner not found")
    
    trend_data = await HealthScoringService.get_health_trend(miner_id, db, days)
    
    return {"miner_id": miner_id, "miner_name": miner.name, "trend": trend_data}


@router.get("/miners/{miner_id}/telemetry/stats", response_model=TelemetryStatsResponse)
async def get_telemetry_stats(
    miner_id: int,
    hours: int = 24,
    db: AsyncSession = Depends(get_db)
):
    """Get aggregated telemetry statistics"""
    result = await db.execute(select(Miner).where(Miner.id == miner_id))
    miner = result.scalar_one_or_none()
    
    if not miner:
        raise HTTPException(status_code=404, detail="Miner not found")
    
    cutoff_time = datetime.utcnow() - timedelta(hours=hours)
    
    # Get telemetry data
    result = await db.execute(
        select(Telemetry)
        .where(Telemetry.miner_id == miner_id)
        .where(Telemetry.timestamp >= cutoff_time)
        .order_by(Telemetry.timestamp.asc())
    )
    telemetry_data = result.scalars().all()
    
    if not telemetry_data:
        raise HTTPException(status_code=404, detail="No telemetry data found")
    
    # Calculate statistics
    hashrates = [t.hashrate for t in telemetry_data if t.hashrate is not None]
    temperatures = [t.temperature for t in telemetry_data if t.temperature is not None]
    powers = [t.power_watts for t in telemetry_data if t.power_watts is not None]
    
    first = telemetry_data[0]
    last = telemetry_data[-1]
    
    accepted_delta = (last.shares_accepted or 0) - (first.shares_accepted or 0)
    rejected_delta = (last.shares_rejected or 0) - (first.shares_rejected or 0)
    total_shares = accepted_delta + rejected_delta
    reject_rate = (rejected_delta / total_shares * 100) if total_shares > 0 else 0
    
    # Calculate uptime
    expected_points = hours * 120  # One every 30 seconds
    actual_points = len(telemetry_data)
    uptime_percent = min((actual_points / expected_points) * 100, 100)
    
    return {
        "avg_hashrate": sum(hashrates) / len(hashrates) if hashrates else None,
        "min_hashrate": min(hashrates) if hashrates else None,
        "max_hashrate": max(hashrates) if hashrates else None,
        "hashrate_unit": last.hashrate_unit if last else "GH/s",
        "avg_temperature": sum(temperatures) / len(temperatures) if temperatures else None,
        "max_temperature": max(temperatures) if temperatures else None,
        "avg_power": sum(powers) / len(powers) if powers else None,
        "total_accepted": accepted_delta,
        "total_rejected": rejected_delta,
        "reject_rate": round(reject_rate, 2),
        "uptime_percent": round(uptime_percent, 2),
        "data_points": len(telemetry_data)
    }


@router.get("/miners/{miner_id}/telemetry/timeseries")
async def get_telemetry_timeseries(
    miner_id: int,
    hours: int = 24,
    metric: str = "hashrate",
    db: AsyncSession = Depends(get_db)
):
    """Get time-series data for charts"""
    result = await db.execute(select(Miner).where(Miner.id == miner_id))
    miner = result.scalar_one_or_none()
    
    if not miner:
        raise HTTPException(status_code=404, detail="Miner not found")
    
    cutoff_time = datetime.utcnow() - timedelta(hours=hours)
    
    result = await db.execute(
        select(Telemetry)
        .where(Telemetry.miner_id == miner_id)
        .where(Telemetry.timestamp >= cutoff_time)
        .order_by(Telemetry.timestamp.asc())
    )
    telemetry_data = result.scalars().all()
    
    # Map metric to field
    metric_map = {
        "hashrate": "hashrate",
        "temperature": "temperature",
        "power": "power_watts"
    }
    
    if metric not in metric_map:
        raise HTTPException(status_code=400, detail=f"Invalid metric. Choose from: {', '.join(metric_map.keys())}")
    
    field = metric_map[metric]
    
    data_points = [
        {
            "timestamp": t.timestamp.isoformat(),
            "value": getattr(t, field)
        }
        for t in telemetry_data
        if getattr(t, field) is not None
    ]
    
    return {
        "miner_id": miner_id,
        "miner_name": miner.name,
        "metric": metric,
        "data": data_points
    }


@router.get("/miners/{miner_id}/export/csv")
async def export_telemetry_csv(
    miner_id: int,
    hours: int = 24,
    db: AsyncSession = Depends(get_db)
):
    """Export telemetry data as CSV"""
    result = await db.execute(select(Miner).where(Miner.id == miner_id))
    miner = result.scalar_one_or_none()
    
    if not miner:
        raise HTTPException(status_code=404, detail="Miner not found")
    
    cutoff_time = datetime.utcnow() - timedelta(hours=hours)
    
    result = await db.execute(
        select(Telemetry)
        .where(Telemetry.miner_id == miner_id)
        .where(Telemetry.timestamp >= cutoff_time)
        .order_by(Telemetry.timestamp.asc())
    )
    telemetry_data = result.scalars().all()
    
    # Create CSV in memory
    output = io.StringIO()
    writer = csv.writer(output)
    
    # Write header
    writer.writerow([
        "Timestamp",
        "Hashrate (GH/s)",
        "Temperature (Â°C)",
        "Power (W)",
        "Shares Accepted",
        "Shares Rejected",
        "Pool"
    ])
    
    # Write data
    for t in telemetry_data:
        writer.writerow([
            t.timestamp.isoformat(),
            t.hashrate or "",
            t.temperature or "",
            t.power_watts or "",
            t.shares_accepted or "",
            t.shares_rejected or "",
            t.pool_in_use or ""
        ])
    
    output.seek(0)
    
    filename = f"{miner.name}_telemetry_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv"
    
    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )


@router.get("/overview/stats")
async def get_overview_stats(db: AsyncSession = Depends(get_db)):
    """Get overview statistics for all miners"""
    from sqlalchemy import func
    result = await db.execute(select(Miner).where(Miner.enabled == True).order_by(func.lower(Miner.name)))
    miners = result.scalars().all()
    
    total_miners = len(miners)
    online_miners = 0
    total_hashrate = 0.0
    avg_temperature = 0.0
    total_power = 0.0
    
    cutoff_time = datetime.utcnow() - timedelta(minutes=5)
    
    for miner in miners:
        # Get latest telemetry
        result = await db.execute(
            select(Telemetry)
            .where(Telemetry.miner_id == miner.id)
            .where(Telemetry.timestamp >= cutoff_time)
            .order_by(Telemetry.timestamp.desc())
            .limit(1)
        )
        latest = result.scalar_one_or_none()
        
        if latest:
            online_miners += 1
            if latest.hashrate:
                total_hashrate += latest.hashrate
            if latest.temperature:
                avg_temperature += latest.temperature
            if latest.power_watts:
                total_power += latest.power_watts
    
    avg_temperature = avg_temperature / online_miners if online_miners > 0 else 0
    
    return {
        "total_miners": total_miners,
        "online_miners": online_miners,
        "offline_miners": total_miners - online_miners,
        "total_hashrate": round(total_hashrate, 2),
        "avg_temperature": round(avg_temperature, 1),
        "total_power": round(total_power, 2)
    }


# ============================================================================
# CKPool Analytics Endpoint (Phase 3)
# ============================================================================

class CKPoolBlockData(BaseModel):
    timestamp: datetime
    block_height: int
    block_hash: str
    effort_percent: float
    time_to_block_seconds: Optional[int]

class CKPoolAnalyticsStats(BaseModel):
    total_blocks: int
    average_effort: float
    median_effort: float
    best_effort: float
    worst_effort: float
    average_time_to_block_hours: Optional[float]
    total_rewards: float
    blocks_24h: int
    blocks_7d: int
    blocks_30d: int

class CKPoolAnalyticsResponse(BaseModel):
    coin: str
    blocks: List[CKPoolBlockData]
    stats: CKPoolAnalyticsStats


@router.get("/ckpool/analytics", response_model=CKPoolAnalyticsResponse)
async def get_ckpool_analytics(
    coin: str = Query(..., description="Coin to filter by (BTC, BCH, DGB)"),
    db: AsyncSession = Depends(get_db)
):
    """Get 12-month CKPool analytics for a specific coin (cached 5 minutes)"""
    from core.database import CKPoolBlockMetrics
    from datetime import timedelta
    import statistics
    
    # Validate coin
    coin = coin.upper()
    if coin not in ["BTC", "BCH", "DGB"]:
        raise HTTPException(status_code=400, detail="Invalid coin. Must be BTC, BCH, or DGB")
    
    # Check cache (5-minute TTL)
    now = datetime.utcnow()
    if coin in _ckpool_analytics_cache:
        cached_time, cached_data = _ckpool_analytics_cache[coin]
        if (now - cached_time).total_seconds() < 300:  # 5 minutes
            return CKPoolAnalyticsResponse(**cached_data)
    
    # Query metrics from last 12 months
    cutoff_date = now - timedelta(days=365)
    result = await db.execute(
        select(CKPoolBlockMetrics)
        .where(CKPoolBlockMetrics.coin == coin)
        .where(CKPoolBlockMetrics.timestamp >= cutoff_date)
        .order_by(CKPoolBlockMetrics.timestamp.desc())
    )
    metrics = result.scalars().all()
    
    if not metrics:
        # Return empty response (and cache it)
        empty_stats = CKPoolAnalyticsStats(
            total_blocks=0,
            average_effort=0.0,
            median_effort=0.0,
            best_effort=0.0,
            worst_effort=0.0,
            average_time_to_block_hours=None,
            total_rewards=0.0,
            blocks_24h=0,
            blocks_7d=0,
            blocks_30d=0
        )
        response_data = {"coin": coin, "blocks": [], "stats": empty_stats.model_dump()}
        _ckpool_analytics_cache[coin] = (now, response_data)
        
        return CKPoolAnalyticsResponse(coin=coin, blocks=[], stats=empty_stats)
    
    # Build blocks array
    blocks = [
        CKPoolBlockData(
            timestamp=m.timestamp,
            block_height=m.block_height,
            block_hash=m.block_hash,
            effort_percent=m.effort_percent,
            time_to_block_seconds=m.time_to_block_seconds
        )
        for m in metrics
    ]
    
    # Calculate statistics
    effort_values = [m.effort_percent for m in metrics]
    time_values = [m.time_to_block_seconds for m in metrics if m.time_to_block_seconds is not None]
    reward_values = [m.confirmed_reward_coins for m in metrics if m.confirmed_reward_coins is not None]
    
    now = datetime.utcnow()
    cutoff_24h = now - timedelta(hours=24)
    cutoff_7d = now - timedelta(days=7)
    cutoff_30d = now - timedelta(days=30)
    
    stats = CKPoolAnalyticsStats(
        total_blocks=len(metrics),
        average_effort=sum(effort_values) / len(effort_values),
        median_effort=statistics.median(effort_values),
        best_effort=min(effort_values),
        worst_effort=max(effort_values),
        average_time_to_block_hours=(sum(time_values) / len(time_values) / 3600) if time_values else None,
        total_rewards=sum(reward_values) if reward_values else 0.0,
        blocks_24h=len([m for m in metrics if m.timestamp >= cutoff_24h]),
        blocks_7d=len([m for m in metrics if m.timestamp >= cutoff_7d]),
        blocks_30d=len([m for m in metrics if m.timestamp >= cutoff_30d])
    )
    
    response_data = {"coin": coin, "blocks": [b.model_dump() for b in blocks], "stats": stats.model_dump()}
    _ckpool_analytics_cache[coin] = (now, response_data)
    
    return CKPoolAnalyticsResponse(coin=coin, blocks=blocks, stats=stats)


class CKPoolHashrateDataPoint(BaseModel):
    """Single hashrate data point"""
    timestamp: datetime
    hashrate_gh: float
    workers: int


class CKPoolHashrateResponse(BaseModel):
    """24-hour hashrate history for a coin"""
    coin: str
    data_points: List[CKPoolHashrateDataPoint]
    total_snapshots: int


@router.get("/ckpool/hashrate", response_model=CKPoolHashrateResponse)
async def get_ckpool_hashrate_history(
    coin: str = Query(..., description="Coin type (BTC, BCH, DGB)"),
    db: AsyncSession = Depends(get_db)
):
    """
    Get 24-hour rolling hashrate history for a CKPool coin.
    Data points captured every 5 minutes.
    """
    from core.database import CKPoolHashrateSnapshot, Pool
    
    # Validate coin
    coin = coin.upper()
    if coin not in ["BTC", "BCH", "DGB"]:
        raise HTTPException(status_code=400, detail="Invalid coin. Must be BTC, BCH, or DGB")
    
    # Get snapshots from last 24 hours for this coin
    cutoff_time = datetime.utcnow() - timedelta(hours=24)
    
    result = await db.execute(
        select(CKPoolHashrateSnapshot)
        .where(CKPoolHashrateSnapshot.coin == coin)
        .where(CKPoolHashrateSnapshot.timestamp >= cutoff_time)
        .order_by(CKPoolHashrateSnapshot.timestamp.asc())
    )
    snapshots = result.scalars().all()
    
    # Aggregate by timestamp if multiple pools for same coin
    # Group by 5-minute bucket to combine multiple pools
    from collections import defaultdict
    time_buckets = defaultdict(lambda: {"hashrate": 0.0, "workers": 0})
    
    for snapshot in snapshots:
        # Round timestamp to 5-minute bucket
        bucket_time = snapshot.timestamp.replace(second=0, microsecond=0)
        minute = (bucket_time.minute // 5) * 5
        bucket_time = bucket_time.replace(minute=minute)
        
        time_buckets[bucket_time]["hashrate"] += snapshot.hashrate_gh
        time_buckets[bucket_time]["workers"] += snapshot.workers
    
    # Build response
    data_points = [
        CKPoolHashrateDataPoint(
            timestamp=ts,
            hashrate_gh=round(data["hashrate"], 2),
            workers=data["workers"]
        )
        for ts, data in sorted(time_buckets.items())
    ]
    
    return CKPoolHashrateResponse(
        coin=coin,
        data_points=data_points,
        total_snapshots=len(data_points)
    )

